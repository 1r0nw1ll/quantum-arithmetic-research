#!/usr/bin/env python3
"""
external_validation_prompt_injection.py

External validation harness for prompt-injection detection in QA Guardrail.

The goal is to validate behavior on a labeled benchmark-style corpus that is
not generated by the validator itself. The harness computes classification
metrics and enforces typed obstruction expectations.
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Any, List


SCRIPT_DIR = Path(__file__).parent
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

from qa_guardrail.qa_guardrail import guard, GuardrailContext  # noqa: E402


DEFAULT_DATASET = SCRIPT_DIR / "external_validation_data" / "prompt_injection_benchmark_subset.jsonl"
OUTPUT_DIR = SCRIPT_DIR / "external_validation_certs"


@dataclass
class PromptCase:
    case_id: str
    benchmark_family: str
    label: str
    prompt: str
    source_note: str

    @property
    def expected_attack(self) -> bool:
        return self.label == "attack"


def _sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        while True:
            chunk = f.read(65536)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def load_cases(path: Path) -> List[PromptCase]:
    if not path.exists():
        raise FileNotFoundError(f"Dataset not found: {path}")

    cases: List[PromptCase] = []
    with path.open("r", encoding="utf-8") as f:
        for lineno, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            try:
                case = PromptCase(
                    case_id=str(obj["case_id"]),
                    benchmark_family=str(obj["benchmark_family"]),
                    label=str(obj["label"]).lower(),
                    prompt=str(obj["prompt"]),
                    source_note=str(obj.get("source_note", "")),
                )
            except KeyError as e:
                raise ValueError(f"Missing field at line {lineno}: {e}") from e

            if case.label not in {"attack", "benign"}:
                raise ValueError(f"Invalid label at line {lineno}: {case.label}")
            cases.append(case)

    if not cases:
        raise ValueError("Dataset is empty")
    return cases


def evaluate_case(case: PromptCase) -> Dict[str, Any]:
    ctx = GuardrailContext(
        active_generators={"sigma", "mu", "lambda", "nu"},
        content=case.prompt,
        policy={"scan_content": True, "deny_on_threats": True},
    )
    result = guard("sigma(1)", ctx)

    predicted_attack = result.result == "DENY"
    fail_type = None
    threats: List[str] = []
    if result.fail_record:
        fail_type = result.fail_record.get("fail_type")
        invariant_diff = result.fail_record.get("invariant_diff", {})
        threats = list(invariant_diff.get("threats", []))

    return {
        "case_id": case.case_id,
        "benchmark_family": case.benchmark_family,
        "label": case.label,
        "expected_attack": case.expected_attack,
        "predicted_attack": predicted_attack,
        "predicted_result": result.result,
        "matched": predicted_attack == case.expected_attack,
        "expected_fail_type_if_attack": "POLICY_CONSTRAINT_VIOLATION",
        "observed_fail_type": fail_type,
        "typed_obstruction_match": (
            (not predicted_attack)
            or (fail_type == "POLICY_CONSTRAINT_VIOLATION")
        ),
        "threat_count": len(threats),
        "threats": threats,
        "source_note": case.source_note,
    }


def summarize(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    tp = tn = fp = fn = 0
    typed_mismatches = 0

    for r in results:
        y = r["expected_attack"]
        yhat = r["predicted_attack"]
        if y and yhat:
            tp += 1
        elif (not y) and (not yhat):
            tn += 1
        elif (not y) and yhat:
            fp += 1
        else:
            fn += 1

        if yhat and not r["typed_obstruction_match"]:
            typed_mismatches += 1

    total = tp + tn + fp + fn
    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall = tp / (tp + fn) if (tp + fn) else 0.0
    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0
    accuracy = (tp + tn) / total if total else 0.0

    by_family: Dict[str, Dict[str, int]] = {}
    for r in results:
        fam = r["benchmark_family"]
        fam_stats = by_family.setdefault(fam, {"total": 0, "correct": 0})
        fam_stats["total"] += 1
        if r["matched"]:
            fam_stats["correct"] += 1

    return {
        "total_cases": total,
        "tp": tp,
        "tn": tn,
        "fp": fp,
        "fn": fn,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy,
        "typed_obstruction_mismatches": typed_mismatches,
        "by_family": by_family,
    }


def run(dataset_path: Path, ci_mode: bool, max_cases: int | None) -> int:
    cases = load_cases(dataset_path)
    if max_cases is not None:
        if max_cases <= 0:
            raise ValueError("--max-cases must be > 0")
        cases = cases[:max_cases]

    results = [evaluate_case(c) for c in cases]
    summary = summarize(results)

    pass_gate = (
        summary["accuracy"] == 1.0
        and summary["recall"] == 1.0
        and summary["fp"] == 0
        and summary["typed_obstruction_mismatches"] == 0
    )

    OUTPUT_DIR.mkdir(exist_ok=True)
    summary_path = OUTPUT_DIR / "prompt_injection_summary.json"
    results_path = OUTPUT_DIR / "prompt_injection_case_results.json"

    payload = {
        "dataset_path": str(dataset_path),
        "dataset_sha256": _sha256_file(dataset_path),
        "summary": summary,
        "gate_passed": pass_gate,
    }

    with summary_path.open("w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

    with results_path.open("w", encoding="utf-8") as f:
        json.dump(results, f, indent=2)

    if ci_mode:
        status = "PASS" if pass_gate else "FAIL"
        print(
            f"[{status}] Prompt injection external validation "
            f"(n={summary['total_cases']}) "
            f"acc={summary['accuracy']:.3f} p={summary['precision']:.3f} "
            f"r={summary['recall']:.3f} f1={summary['f1']:.3f} "
            f"typed_mismatch={summary['typed_obstruction_mismatches']}"
        )
        return 0 if pass_gate else 1

    print("=" * 78)
    print("PROMPT INJECTION EXTERNAL VALIDATION")
    print("=" * 78)
    print(f"Dataset: {dataset_path}")
    print(f"Cases:   {summary['total_cases']}")
    print()
    print("Metrics")
    print(f"  Accuracy:  {summary['accuracy']:.3f}")
    print(f"  Precision: {summary['precision']:.3f}")
    print(f"  Recall:    {summary['recall']:.3f}")
    print(f"  F1:        {summary['f1']:.3f}")
    print(f"  TP/TN/FP/FN: {summary['tp']}/{summary['tn']}/{summary['fp']}/{summary['fn']}")
    print(f"  Typed obstruction mismatches: {summary['typed_obstruction_mismatches']}")
    print()
    print("Family breakdown")
    for fam, fam_stats in sorted(summary["by_family"].items()):
        print(f"  {fam}: {fam_stats['correct']}/{fam_stats['total']} correct")
    print()
    print("Gate verdict:", "PASS" if pass_gate else "FAIL")
    print(f"Summary: {summary_path}")
    print(f"Cases:   {results_path}")

    return 0 if pass_gate else 1


def main() -> int:
    parser = argparse.ArgumentParser(description="QA prompt-injection external validation harness")
    parser.add_argument(
        "--dataset",
        type=str,
        default=str(DEFAULT_DATASET),
        help="Path to JSONL labeled dataset",
    )
    parser.add_argument("--ci", action="store_true", help="CI mode: single-line output")
    parser.add_argument(
        "--max-cases",
        type=int,
        default=None,
        help="Optional cap on number of cases for quick runs",
    )
    args = parser.parse_args()

    env_max = os.environ.get("QA_PI_MAX_CASES")
    max_cases = args.max_cases
    if env_max is not None and max_cases is None:
        max_cases = int(env_max)

    return run(Path(args.dataset), ci_mode=args.ci, max_cases=max_cases)


if __name__ == "__main__":
    sys.exit(main())
