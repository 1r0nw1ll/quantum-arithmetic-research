% Results: QA as Near One-Shot Structural Learning Across Domains
\section{QA as Near One-Shot Structural Learning Across Domains}

\paragraph{Premise.} QA provides a compact geometric prior: from any datum $x$ we compute a small invariant block $\Phi(x)$ (e.g., $b,e,d,a,J,X,K,C,F,G$) encoding ellipse/triangle structure and modular phases. In this space, communities and margins emerge with minimal computation; learning reduces to light param fitting.

\subsection{Graphs: Football and QA Knowledge Graph}
\label{subsec:graphs}
On the college football network (ground truth conferences known), a single spectral pass with QA-X weighting improves modularity and maintains (slightly improves) alignment over both the unweighted baseline and Louvain (Table~\ref{tab:football}). On the QA knowledge graph (no labels), QA weighting converts a near-flat baseline (Q~\,0.0004) into strong community structure (Q~\,0.505 for QA-X; Q~\,0.72 for the full QA kernel), see Table~\ref{tab:qakg}.

\subsection{Synthetic Manifolds}
\label{subsec:manifolds}
On two moons and swiss roll, QA per-feature mapping straightens manifolds, enabling classical learners to succeed with fewer labels and epochs. Logistic Regression reaches higher accuracy at each train size on moons; a tiny SGD(logistic) model attains target accuracy in fewer epochs (figures: moons\_logreg\_curve, moons\_mlp\_epochs). For unsupervised clustering (KMeans), ARI/NMI improve on swiss roll and are comparable on moons (Table~\ref{tab:manifolds-kmeans}); clustering performance depends on the downstream algorithm, whereas margins benefit consistently.

\subsection{Real Signals (Tabular/Raman)}
\label{subsec:tabular}
On a synthetic tabular task (20 features), QA and raw+QA representations converge faster than raw under the same optimizer and model capacity (tabular\_sgd\_epochs). This mirrors our Raman pipeline behavior: QA blocks reduce epochs-to-accuracy and remain robust with fewer labels, supporting the claim that QA compresses structural complexity so param fitting becomes few-shot.

\paragraph{Conclusion.} Across graphs, manifolds, and tabular signals, QA exposes latent structure with a single structural step (spectral/clustering/margin). The remaining learning is shallow and fast, consistent with QA as a universal structural prior.

